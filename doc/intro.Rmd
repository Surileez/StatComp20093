---
title: "Homework"
author: "Siyu Li"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to StatComp}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
NULL
## 2020-09-27  

## Question 1

Here is a table containing weight and height of 15 women. Do linear regression.

## Answer 1
```{r}
lm.WH<-lm(women$weight~women$height)
summary(lm.WH)$coef
```
The $R^2$ is `r summary(lm.WH)$r.squared`
```{r}
#par(mfrow=c(2,2))
plot(lm.WH)
```

## Question 2
Show the table.

## Answer 2
```{r results='asis'}
print(xtable::xtable(head(women)),type='html')
```

## Question 3
Write the formula of normal distribution function.

## Answer 3
$$F(x)=\int_{0}^{x}\frac{1}{\sigma\sqrt{2\pi}}e^{- \frac{(x-\mu)^2}{2\sigma^2}}\text{d}t$$   
   
    
## 2020-09-29

## 3.3

The Pareto(a,b) distribution has cdf  
$$F(x)=1-(\frac{b}{x})^{a},x\geq b>0,a>0$$  
Derive the probability inverse transformation $F^{-1}(U)$ and use the inverse transform method to simulate a random sample from the Pareto(2,2) distribution. Graph the density histogram of the sample with the Pareto(2,2) density superimposed for comparison.

## Answer 1
```{r}
n <- 1000 
u <- runif(n) 
x <- 2/((1-u)^0.5) # F(x) =1-(2/x)^2,x>=2
hist(x, prob = TRUE, main = expression(f(x)==8/x^3),
xlim=c(0,20),breaks=100)
y<-seq(0,20,.1)
lines(y,8/(y^3),col='blue')
```

## 3.9
The rescaled Epanechnikov kernel[85] is a symmetric density function
$$f_{c}(x)=\dfrac{3}{4}(1-x^{2})$$
Devroyc and Gyorli[71,p.236] give the following algorithm for simulation from the distribution. Generate iid $U_{1},U_{2},U_{3}$ $\sim$ $U(-1,1)$. If $|U_{3}|\geq |U_{2}|$ and $|U_{3}|\geq |U_{1}|$,deliver $U_{2}$; otherwise deliver $U_{3}$. Write a function to generate random variates from $f_{c}$, and construct the histogram density estimate of a large simulated random sample.


## Answer 2
```{r}
n <- 10000 
u1 <- runif(n,-1,1) 
u2 <- runif(n,-1,1) 
u3 <- runif(n,-1,1) 
x<-ifelse(abs(u3)>=abs(u2)&abs(u3)>=abs(u1),u2,u3)
hist(x, prob = TRUE, main = expression(f(x)==3(1-x^2)/4),breaks=50)
y<-seq(-1,1,.1)
lines(y,3*(1-y^2)/4,col='blue')
```

## 3.10
Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_{c}$.

## Answer 3
根据均匀分布性质可知  
$P(U_{i}=x)=\dfrac{1}{2},P(|U_{i}|=x)=1,0\leq x\leq 1$  
记事件A={$|U_{3}|\geq |U_{2}|$&$|U_{3}|\geq |U_{1}|$}
\begin{align*}
P(X=k)&=P(U_{2}=k|A)P(A)+P(U_{3}=k|A^{c})P(A^c)\\  
&=P(A|U_{2}=k)P(U_{2}=k)+P(A^{c}|U_{3}=k)P(U_{3}=k)\\  
&=P(|U_{3}|\geq |k|,|U_{3}|\geq |U_{1}|)\dfrac{1}{2}+(1-P(|U_{1}|,|U_{2}|\leq |k|))\dfrac{1}{2}\\  
&=\dfrac{1}{2}(\int_{|k|}^{1}P(|U_{3}|=v)P(|U_{3}|\leq v)dv+(1-|k|^{2}))\\  
&=\dfrac{1}{2}(\int_{|k|}^{1}vdv+(1-k^{2}))\\  
&=\dfrac{1}{2}(\dfrac{1-k^{2}}{2}+(1-k^{2}))\\  
&=\dfrac{3}{4}(1-k^{2})\\  
&=f_{c}(k),0\leq k\leq 1
\end{align*}
The algorithm given in Exercise 3.9 generates variates from the density $f_{c}$.


## 3.13
It can be shown that the mixture in Exercise 3.12 has a Pareto distribution with cdf  
$$F(y)=1-(\dfrac{\beta}{\beta +y})^{r}, y\geq 0$$
(This is an alternative parameterization of the Pareto cdf given in Exercise3.3) Generate 1000 random observations from the mixture with $r=4$ and $\beta=2$. Compare the empirical and theoretical(Pareto)dustributions by graphing the density histogram of the sample and superimposing the Pareto density curve.

## Answer 4
```{r}
n <- 1000 
u <- runif(n) 
x <- 2/((1-u)^0.25)-2 # F(x)=1-(2/(2+x))^4,x>=0
hist(x, prob = TRUE, main = expression(f(x)==64/(2+x)^5),
xlim=c(0,10),breaks=100)
y<-seq(0,10,.1)
lines(y,64/((2+y)^5),col='blue')
```


  
## 2020-10-13  
  
## 5.1
Compute a Monte Carlo estimate of
$$\int_{0}^{\pi/3}sintdt$$
and compare the estimate with the exact value of the integral.

## Answer 1
```{r}
n <- 10000; 
t <- runif(n, min=0, max=pi/3) #t~U(0,pi/3)
theta_hat<- mean(pi/3*sin(t)) #Monte Carlo estimate 
print(c(theta_hat,-cos(pi/3)+cos(0)))#compare
```
The Monte Carlo estimate and the exact value of the integral are close.

## 5.7
Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.
$$\theta=\int_{0}^{1}e^{x}dx$$

## Answer 2
```{r}
antithetic<-function(n,anti=TRUE){
  x1 <- runif(n);
  u <- x1[1:n/2]
  v <- 1-u;
  x2<- c(u,v)
  if(anti)
  return (mean(exp(x2)))
  else
  return (mean(exp(x1)))
}

n <- 10000; 
#antithetic variate approach
theta_anti<-antithetic(n) 
#simple Monte Carlo method
theta_simple<-antithetic(n,anti=FALSE) 
m<-1000
MC_anti<-MC_simple<-numeric(m)
for(i in 1:m){
  MC_anti[i]<-antithetic(n);
  MC_simple[i]<-antithetic(n,anti=FALSE)
}
#Compare the result with the theoretical value
print(c(theta_anti,theta_simple,exp(1)-exp(0)))
#Compute an empirical estimate of the percent reduction in variance using the antithetic variate
print(c(sd(MC_anti),sd(MC_simple),sd(MC_anti)/sd(MC_simple),1-sd(MC_anti)/sd(MC_simple)))
```
Compare the results with the theoretical value and we find they are close.   
An empirical estimate of the percent reduction in variance using the antithetic variate is `r 1-sd(MC_anti)/sd(MC_simple)`.

## 5.11
If $\hat{\theta_{1}}$ and $\hat{\theta_{2}}$ are unbiased estimators of $\theta$, and $\theta_{1},\theta_{2}$are antithetic, we derived that $c^{*}=1/2$ is the optimal constant that minimizes the variance of $\hat\theta_{c}=c\hat\theta_{1}+(1-c)\hat\theta_{2}$. Derive $c^{*}$ for the general case. That is, if $\hat\theta_{1}$,$\hat\theta_{2}$ are any two unbiased estimators of $\theta$, find the value $c^{*}$ that minimizes the variance of the $\hat\theta_{c}=c\hat\theta_{1}+(1-c)\hat\theta_{2}$ in equation(5.11).($c^{*}$ will be a function of the variances and the covariance of the estimators.)

## Answer 3
  \begin{align*}
  Var\hat\theta_{c}&=Var(c\hat\theta_{1}+(1-c)\hat\theta_{2})\\    
  &=Var(c\hat\theta_{1})+Var((1-c)\hat\theta_{2})+2Cov(c\hat\theta_{1},(1-c)\hat\theta_{1})\\  
  &=c^{2}Var\hat\theta_{1}+(1-c)^2Var\hat\theta_{2}+2c(1-c)Cov(\hat\theta_{1},\hat\theta_{2})\\  
  &=(Var\hat\theta_{1}+Var\hat\theta_{2}-2Cov(\hat\theta_{1},\hat\theta_{2}))c^{2}+2(Cov(\hat\theta_{1},\hat\theta_{2})-Var\hat\theta_{2})c+Var\hat\theta_{2}\\  
  \end{align*}
$c^{*}=\dfrac{Var\hat\theta_{2}-Cov(\hat\theta_{1},\hat\theta_{2})}{Var\hat\theta_{1}+Var\hat\theta_{2}-2Cov(\hat\theta_{1},\hat\theta_{2})}=\dfrac{Cov(\hat\theta_{2}-\hat\theta_{1},\hat\theta_{2})}{Var(\hat\theta_{1}-\hat\theta_{2})}$ is the optimal constant that minimizes $Var\hat\theta_{c}$.
  
## 2020-10-20  

## 5.13
Find two importance functions $f_{1}$ and $f_{2}$ that are supported on $(1,\infty)$ and are 'close' to
$$g(x)=\dfrac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2},x>1$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_{1}^{\infty}\dfrac{x^{2}}{\sqrt{2\pi}}e^{-x^{2}/2}dx$$
by importance sampling?Explain.  


## Answer 1
Let $f1(x)=\dfrac{dnorm(x,\mu=0,sd=1)}{1-pnorm(1)}$, is almost normal distribution but supported on $(1,\infty)$  
Let $f2(x)=\dfrac{dexp(x,\lambda=0.5)}{1-pexp(1)}$，is almost exponential distribution but supported on $(1,\infty)$
```{r}
m<- 1e4
g<-function(x){x^2/sqrt(2*pi)*exp(-x^2/2)*(x>1)}
#choose f1 as importance sampling
x<-rnorm(m)
fg1<-g(x)/dnorm(x)
theta.hat1<-mean(fg1)
#choose f2 as important sampling
x<-rexp(m,1/2)
fg2<-g(x)/dexp(x,1/2)
theta.hat2<-mean(fg2)
print(c(theta.hat1,theta.hat2))#theta results
integrate(g,1,Inf)#true integral
print(c(sd(fg1),sd(fg2)))#variance
```
We find that the two results of $\hat\theta$ are both close to the true integral. And $f2$ produce the smaller variance. By compute the equation below, we can get the conclusion.
$$Var(\hat\theta)=\int_{A}\dfrac{g^{2}(x)}{f(x)}dx-\theta^{2}$$
\  
\  


## 5.15
Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.  


## Answer 2
According to Example 5.10, we choose $f(x)=\dfrac{e^{-x}}{1-e^{-x}}$ as importance sampling.  
While consider each $[\dfrac{j-1}{5},\dfrac{j}{5}]$, we adjust importance sampling to $f_{j}(x)=\dfrac{e^{-x}}{e^{-\dfrac{j-1}{5}}-e^{-\dfrac{j}{5}}}, x \in [\dfrac{j-1}{5},\dfrac{j}{5}]$
```{r}
M <- 10000
N <- 50 
k <- 5 
r <- M/k 
T5 <- numeric(k)
est <- matrix(0, N, 2)
#use reverse transform method
g<-function(x,a,b) exp(-x)/(1+x^2)*(x>a)*(x<b)
h<-function(u,a,b) -log(exp(-a)-u*(exp(-a)-exp(-b)))
fg<-function(x,a,b) g(x,a,b)/(exp(-x)/(exp(-a)-exp(-b)))
for (i in 1:N) {
u<-runif(M)
u.s<-runif(M/k)
#importance sampling
est[i, 1] <- mean(fg(h(u,0,1),0,1))
#stratified importance sampling
for(j in 1:k) T5[j]<-mean(fg(h(u.s,(j-1)/k,j/k),(j-1)/k,j/k))
est[i, 2] <- sum(T5)
}
apply(est,2,mean)
apply(est,2,sd)
```
The results of $\hat\theta$ are close. Compare the variance, we find the variance using stratified importance sampling is smaller. 

\  
\  


## 6.4
Suppose that $X_{1},...,X_{n}$ are a random sample from a lognormal distribution with unknown parameters. Construct a 95% confidence interval for the parameter $\mu$. Use a Monte Carlo method to obtain an empirical estimate of the confidence level.  

## Answer 3

(1)Construct a 95% confidence interval for the parameter $\mu$.
$$ln(X)\sim N(\mu,\sigma^{2})$$
$$\dfrac{ln(X)-\mu}{S/\sqrt{n}}\sim t(n-1)$$
$$\dfrac{(n-1)S^{2}}{\sigma^{2}}\sim \chi^{2}(n-1)$$
95% confidence interval for the parameter $\mu$ is
$$[\overline {ln(X)}-\dfrac{S}{\sqrt{n}}t_{0.025}(n-1),\overline{ln(X)}+\dfrac{S}{\sqrt{n}}t_{0.025}(n-1))],where\  S^2=Var(ln(X))$$
(2)Use a Monte Carlo method to obtain an empirical estimate of the confidence level.  
For each replicate, indexed j=1,..,m  
First,Generate the $j^{th}$ random number $X_{1}^{j},..X_{n}^{j}$   
Next,Compute the 95% confidence interval $C_{j}$ for the $j^{th}$ sample  
Then,Compute empirical estimate of the confidence level $y=\dfrac{1}{m}\sum_{j=1}^{m} I(\mu \in C_{j})$

\  
\  


## 6.5
Suppose a 95% symmetric $t$-interval is applied to estimate a mean, but the sample data are unnormal. Then the probability that the confidence interval covers the mean is not necessary equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the $t$-interval for random sample of $\chi^{2}(2)$ data with sample size $n=20$. Compare your $t$-interval results with the simulation results in Example 6.4.(The $t$-interval should be more robust to departures from nomality than the interval for variance.)  


## Answer 4
According to CLT,  we assume 95% confidence interval for the parameter $\mu$ is
$$[\overline X-\dfrac{S}{\sqrt{n}}t_{0.025}(n-1),\overline{X}+\dfrac{S}{\sqrt{n}}t_{0.025}(n-1))]$$
```{r}
n<-20
alpha<-0.05
#symmetric t-interval
t_UCL<-replicate(1000, expr = { 
  x <- rchisq(n,2)
  c(mean(x),(var(x)/n)^(0.5)*qt(1-alpha/2, df = n-1)) })
#Example 6.4, use chisq sample by assuming it as normal distribution
UCL <- replicate(1000, expr = { 
  x <- rchisq(n,2)
  (n-1) * var(x) / qchisq(alpha, df = n-1) })
#Example 6.4, use normal distribution sample
UCLN <- replicate(1000, expr = { 
  x <- rnorm(n,0,2)
  (n-1) * var(x) / qchisq(alpha, df = n-1) })
#coverage probability
mean((t_UCL[1,]+t_UCL[2,])>2&(t_UCL[1,]-t_UCL[2,])<2)
mean(UCL > 4)
mean(UCLN > 4)
```
Compare the results, we find that the coverage probability of symmetric $t$-interval is closer to 0.95 than using the $\chi^{2}$ sample by assuming it as normal distribution to estimate. Because it's not normal distribution, the results are both less closer to 0.95 compared to the condition that the sample is from normal distribution.
  
## 2020-10-27
  
## 6.7
Estimate the power of the skewness test of normality against symmetric $Beta(\alpha,\alpha)$ distributions and comment on the results. Are the results different for heavy-tailed symmetric alternatives such as $t(ν)$?

## Answer 1
```{r warning=FALSE}
sk <- function(x) {
#computes the sample skewness coeff.
xbar <- mean(x)
m3 <- mean((x - xbar)^3)
m2 <- mean((x - xbar)^2)
return( m3 / m2^1.5 )
}
n <- 30
m <- 2500
a<-0.1
alpha <- c(seq(1, 100, 1))
N <- length(alpha)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qnorm(1-a/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
for (j in 1:N) { #for each epsilon
e <- alpha[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
x <- rbeta(n, e, e)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pwr[j] <- mean(sktests)
}
#plot power vs alpha
plot(alpha, pwr, type = "b",
xlab = bquote(alpha))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(alpha, pwr+se, lty = 3)
lines(alpha, pwr-se, lty = 3)
```
```{r}
n <- 30
m <- 2500
a<-0.1
v<- c(seq(1, 100, 1))
N <- length(v)
pwr <- numeric(N)
#critical value for the skewness test
cv <- qnorm(1-a/2, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
for (j in 1:N) { #for each epsilon
e <- v[j]
sktests <- numeric(m)
for (i in 1:m) { #for each replicate
x <- rt(n, e)
sktests[i] <- as.integer(abs(sk(x)) >= cv)
}
pwr[j] <- mean(sktests)
}
#plot power vs v
plot(v, pwr, type = "b",
xlab = bquote(v))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(v, pwr+se, lty = 3)
lines(v, pwr-se, lty = 3)
```
   
As for $Beta(\alpha,\alpha)$, pwr is less than 0.1 when alpha is small. It is almost monotonic increasing.When alpha is close to infinity, pwr is closer to 0.1. That can be illustrated because $Beta(\alpha,\alpha)$ distribution is close to normality.  
As for heavy-tailed symmetric alternatives such as $t(ν)$, the trend of the line is reverse. It is almost monotonic decreasing. When $v$ is close to infinity, pwr is closer to 0.1. It matches with the conclusion that when $v\to \infty$, $t(v)$ is normal distribution.  
Because both two distribution are symmetric, value of skewness can be seen as 0 under some situations.  


## 6.8
Refer to Example 6.16. Repeat the simulation, but also compute the F test of equal variance, at significance level $\hat\alpha=0.055$. Compare the power of the Count Five test and F test for small, medium, and large sample sizes. (Recall that the $F$ test is not applicable for non-normal distributions.)

## Answer 2
```{r warning=FALSE}
#Count Five test
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
# generate samples under H1 to estimate power
sigma1 <- 1
sigma2 <- 1.5
m <- c(10,500,100000)
power<-length(m)
for(i in 1:3){
power[i] <- mean(replicate(m[i], expr={
x <- rnorm(20, 0, sigma1)
y <- rnorm(20, 0, sigma2)
count5test(x, y)
}))
}
print(power)
```
```{r}
#F test
sigma1 <- 1
sigma2 <- 1.5
alpha<-0.055
m <- c(10,500,100000)
power<-length(m)
for(i in 1:3){
power[i] <- mean(replicate(m[i], expr={
x <- rnorm(20, 0, sigma1)
y <- rnorm(20, 0, sigma2)
as.integer(var.test(x,y)$p.value <= alpha)
}))
}
print(power)
```
Compare the power of the Count Five test and F test for small, medium, and large sample sizes, we find that the larger sample size makes the power be closer to the confidence level alpha. F-test is more powerful.   


## 6.C
Repeat Examples 6.8 and 6.10 for Mardia’s multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If $X$ and $Y$ are $iid$, the multivariate population skewness $β_{1,d}$is deﬁned by Mardia as 
$$β_{1,d}= E[(X −µ)^{T}Σ^{−1}(Y −µ)]^3$$ 
Under normality, $β_{1,d}= 0$. The multivariate skewness statistic is
$$b_{1,d} =\dfrac{1}{n^2}\Sigma_{i,j=1}^{n} 
((X_{i} − \bar X)^T\hatΣ^{−1}(X_{j} − \bar X))^3, (6.5)$$
where $\hat\Sigma$ is the maximum likelihood estimator of covariance. Large values of $b_{1,d}$ are significant. The asymptotic distribution of $nb_{1,d}/6$ is chisquared with $d(d +1)(d +2)/6$ degrees of freedom.

## Answer 3
```{r}
#6.8
#Consider multiple normal distribution
library(MASS)
multi_sk <- function(X){
n <- nrow(X)
xbar <- colMeans(X)
sigma.hat <- cov(X) * (n - 1) / n
b <- sum(((t(t(X) - xbar))%*%solve(sigma.hat)%*%(t(X) - xbar))^3) / n^2
return (b)
}
d=2 # the dimension
n=c(10,20,30,50,100,500) # sample sizes
cv=qchisq(0.95,d*(d+1)*(d+2)/6)
p.reject=numeric(length(n)) 
m=1000 
sig<-matrix(c(10,0,0,10),nrow=2)
for (i in 1:length(n)) {
  sktests=numeric(m) 
  for (j in 1:m) {
    x=mvrnorm(n[i],rep(0,d),sig) 
    sktests[j]=as.integer(abs(n[i]*multi_sk(x)/6) >= cv )
  }
  p.reject[i]=mean(sktests) #proportion rejected
}
p.reject
```
As sample size increases, the answer increases, and it is closer to alpha.

```{r}
#6.10
d<-2
library(MASS)
alpha <- .1
n <- 30
m <- 500
epsilon <- c(seq(0, .15, .02), seq(.15, 1, .05))
N <- length(epsilon)
pwr <- numeric(N)
cv<-qchisq(1-alpha,d*(d+1)*(d+2)/6)
for (j in 1:N) { #for each epsilon
e <- epsilon[j]
sktests <- numeric(m)
 for (i in 1:m) { #for each replicate
 sigma <- sample(c(1, 10), replace = TRUE,
 size = n, prob = c(1-e, e))
 x=matrix(0,n,d)
  for(k in 1:n){
  x[k,] <- mvrnorm(1, rep(0,2), diag(sigma[k]^2,d))}
 sktests[i] <- as.integer(n*abs(multi_sk(x))/6 >= cv)
}
pwr[j] <- mean(sktests)
}
#plot power vs epsilon
plot(epsilon, pwr, type = "b",
xlab = bquote(epsilon), ylim = c(0,1))
abline(h = .1, lty = 3)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(epsilon, pwr+se, lty = 3)
lines(epsilon, pwr-se, lty = 3)
```


## Discussion I 
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level? 
What is the corresponding hypothesis test problem?
What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? 
What information is needed to test your hypothesis?

## Answer
We can't say the powers are different at 0.05 level simply.  
1.We need do comparison.
$$H_{0}:po1=p2\leftrightarrow H_{1}:p1 \not= p2.$$
2.The two-sample can't be used because the samples are not independent.  
3.If we use McNemar test, we need to know information as below:  
(1)rejecting method 1 but accepting method 2;   (2)rejecting method 2 but accepting method 1.

## 2020-11-03
  
## 7.1
Compute a jackknife estimate of the bias and the standard error of the correlation statistic in Example 7.2.

## Answer 1
```{r waring=FALSE}
library(bootstrap)
n <- nrow(law) #sample size
theta_hat <- cor(law$LSAT, law$GPA)
theta_j <- numeric(n)
for (i in 1:n) {
#randomly select the indices
  x<-law[-i,]
  LSAT <- x$LSAT
  GPA <- x$GPA
  theta_j[i] <- cor(LSAT, GPA)
}
bias_jack <- (n - 1) * (mean(theta_j) - theta_hat) 
#jackknife bias
se_jack <- (n - 1) * sqrt(var(theta_j) / n)
#jackknife standard error 
cat('\n','Jackknife bias',bias_jack,'\n', 'Jackknife standard error ',se_jack)
```

## 7.5
Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the
mean time between failures 1/λ by the standard normal, basic, percentile,
and BCa methods. Compare the intervals and explain why they may differ.

## Answer 2
We assume $X\sim exp(\lambda)$, thus, $\theta=E(X)=\dfrac{1}{\lambda}$.
```{r warning=FALSE}
library(boot)
mean(aircondit[,1])#theta
N=200#boot sample size
bootstrap_result<-boot(aircondit,statistic= function(x,ind){mean(x[ind,1])},R=N)
boot.ci(bootstrap_result, conf = 0.95, type = 'all')
```
Standard normal:
$$Z=\dfrac{\hat\theta-E\hat\theta}{se\hat\theta}$$
n\to infty,according to Theorm CLL，Z\sim N(0,1).Assume it is unbiased,95% bootstrap confidence intervals is
$$[\theta \pm z_{0.025}\hat se(\hat\theta)]$$
Basic:  
$$P((\hat\theta-\theta)_{\alpha/2}<\hat\theta-\theta<(\hat\theta-\theta)_{1-\alpha/2})=1-\alpha$$
We use $\hat\theta^*-\theta$ to estimate $\hat\theta-\theta$
$$P((\hat\theta^*-\theta)_{\alpha/2}<\hat\theta-\theta<(\hat\theta^*-\theta)_{1-\alpha/2})=1-\alpha$$
95% bootstrap confidence intervals is
$$[2\hat\theta-\hat\theta_{1-\alpha/2}^*,2\hat\theta-\hat\theta_{\alpha/2}^*]$$
BCa:    
Bias-corrected compared to percentile method.  
The four methods get different computing results related to bootstrap samples. Thus, they differ, as the bootstrap samples differ.

## 7.8
Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard
error of $\hatθ$.

## Answer 3
```{r waring=FALSE}
library(bootstrap)
library(boot)
lambda_hat<-eigen(cov(scor))$values
theta_hat <- lambda_hat[1] / sum(lambda_hat) 
N <- 200 #sample size
n <- nrow(scor) 
# Jackknife 
theta_j <- rep(0, n) 
for (i in 1:n) { 
   x <- scor [-i,] 
   lambda <- eigen(cov(x))$values 
   theta_j[i] <- lambda[1] / sum(lambda) 
   } 
bias_jack <- (n - 1) * (mean(theta_j) - theta_hat) 
#jackknife bias
se_jack <- (n - 1) * sqrt(var(theta_j) / n)
#jackknife standard error
cat('\n','Jackknife bias',bias_jack,'\n','Jackknife standard error',se_jack )
```

## 7.11
In Example 7.18, leave-one-out (n-fold) cross validation was used to select the
best fitting model. Use leave-two-out cross validation to compare the models.

## Answer 4
1. Linear: $Y = β_0 + β_1X + ε$.
2. Quadratic: $Y = β_0 + β_1X + β_2X^2 + ε$.
3. Exponential: $log(Y) = log(β_0) + β_1X + ε$.
4. Log-Log: $log(Y) = β_0 + β_1log(X) + ε$.
```{r include=FALSE}
library(DAAG)
attach(ironslag)
```
```{r warning=FALSE}
n <- length(magnetic) #in DAAG ironslag
e1 <- e2 <- e3 <- e4 <- numeric(n*(n-1)/2)
t=1
a <- seq(10, 40, .1) #sequence for plotting fits
# for n-fold cross validation
# fit models on leave-two-out samples
for (k in 1:(n-1)) 
 for(j in (k+1):n){
   y <- magnetic[c(-k,-j)]
   x <- chemical[c(-k,-j)]
   J1 <- lm(y ~ x)
   yhat1 <- J1$coef[1] + J1$coef[2]*chemical[c(k,j)]
   e1[t] <- mean(magnetic[c(k,j)]-yhat1)
   J2 <- lm(y ~ x + I(x^2))
   yhat2 <- J2$coef[1] + J2$coef[2] * chemical[c(k,j)] +J2$coef[3] * chemical[c(k,j)]^2
   e2[t] <- mean(magnetic[c(k,j)] - yhat2)
   J3 <- lm(log(y) ~ x)
   logyhat3 <- J3$coef[1] + J3$coef[2] * chemical[c(k,j)]
   yhat3 <- exp(logyhat3)
   e3[t] <- mean(magnetic[c(k,j)] - yhat3)
   J4 <- lm(log(y) ~ log(x))
   logyhat4 <- J4$coef[1] + J4$coef[2] * log(chemical[c(k,j)])
   yhat4 <- exp(logyhat4)
   e4[t] <- mean(magnetic[c(k,j)] - yhat4)
   t=t+1
}
c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2))

```
We also consider L2 as the best fitting model, because mean(e2^2) is smallest. The result is same as it in Example 7.18. However, the bias is smaller by using leave-two-out validation. This is expected.
```{r}
L2 <- lm(magnetic ~ chemical + I(chemical^2))
L2
```

```{r}
detach(ironslag)
```



## 2020-11-10  
  
## 8.3

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for
equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

## Answer 1
```{r}
count5test <- function(x, y) {
X <- x - mean(x)
Y <- y - mean(y)
outx <- sum(X > max(Y)) + sum(X < min(Y))
outy <- sum(Y > max(X)) + sum(Y < min(X))
# return 1 (reject) or 0 (do not reject H0)
return(as.integer(max(c(outx, outy)) > 5))
}
n1 <- 20;n2 <- 30#sample sizes are not equal
mu1 <- mu2 <- 0;sigma1 <- sigma2 <- 1
m <- 1000;R <- 999;K <- 1:50
reps <- numeric(R)
tests <- replicate(m, expr = {
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
z <-c(x,y)
for (i in 1:R) {
k <- sample(K, size = n1, replace = FALSE)
x1 <- z[k]
y1 <- z[-k] #complement of x1
k <- sample(1:n2, size = n1, replace = FALSE)
y1 <- y1[k] #control the sample size
reps[i] <- count5test(x1, y1)
}
mean(reps)
} )
alphahat <- mean(tests)
alphahat
```
$X_1,...X_{20} \sim N(0,1),Y_1,...Y_{30}\sim N(0,1)$.   
We use permutation test to select equal sample sizes. Then, we can apply count five test. In the simulation, each sample was centered by subtracting the sample mean, and the empirical Type I error rate was `r alphahat`. The adjusted “Count Five” criterion  controls Type I error at $α ≤ 0.0625$ when the sample sizes are unequal. As we generated data from the same normal distribution $N(0,1)$, this result is rational.

## Question 2
Design experiments for evaluating the performance of the NN,energy, and ball methods in various situations.  
1.Unequal variances and equal expectations
```{r warning=FALSE}
library(RANN)
library(energy)
library(boot)
Tn <- function(z, ix, sizes,k) {
n1 <- sizes[1]; n2 <- sizes[2]; n <- n1 + n2
if(is.vector(z)) z <- data.frame(z,0);
z <- z[ix, ];
NN <- nn2(data=z, k=k+1) 
block1 <- NN$nn.idx[1:n1,-1]
block2 <- NN$nn.idx[(n1+1):n,-1]
i1 <- sum(block1 < n1 + .5); i2 <- sum(block2 > n1+.5)
(i1 + i2) / (k * n)
}
```
```{r warning=FALSE,include=FALSE}
library(RANN)
library(energy)
library(Ball)
library(boot)
library(stargazer)
```
```{r warning=FALSE}
m <- 100; k<-3; set.seed(12345)
mu1 <- mu2 <- 0#equal expectations
sigma1 <-1; sigma2 <- 2#unequal variances 
n1 <- n2 <- 20; R<-999; n <- n1+n2; N = c(n1,n2)
eqdist.nn <- function(z,sizes,k){
boot.obj <- boot(data=z,statistic=Tn,R=R,
sim = "permutation", sizes = sizes,k=k)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
p.values <- matrix(NA,m,3)
for(i in 1:m){
x <- rnorm(n1, mu1, sigma1)#N(0,1)
y <- rnorm(n2, mu2, sigma2)#N(0,2)
z <- c(x, y) 
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
alpha <- 0.1;
pow <- colMeans(p.values<alpha)
names(pow)<-c('NN','energy','Ball')
```
```{r results='asis'}
stargazer(pow,type='html')
```
Under this circumstance:$X\sim N(0,1),Y\sim N(0,2)$, Ball is the most powerful, due to Ball could be more powerful for non-location family distribution(eg:the variances are different).  
  
2.Unequal variances and unequal expectations
```{r waring=FALSE}
mu1<-0;mu2<-1#unequal expectations
sigma1 <-1;sigma2<-2#unequal variances 
set.seed(12345)
for(i in 1:m){
x <- rnorm(n1, mu1, sigma1)#N(0,1)
y <- rnorm(n2, mu2, sigma2)#N(1,2)
z <- c(x, y) 
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
pow <- colMeans(p.values<alpha)
names(pow)<-c('NN','energy','Ball')
```
```{r results='asis'}
stargazer(pow,type='html')
```
We reset the expectations to be unequal. Then, $X\sim N(0,1),Y\sim N(1,2)$, Ball is the most powerful, while energy is also powerful. This is rational, due to Ball could be more powerful for unequal variances, while energy could be more powerful for unequal expectations.    
    
3.Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions)
```{r warning=FALSE}
set.seed(12345)
rnd <- function(n){
      s <- rbinom(n, 1, 0.5)                   
      S <- rnorm(n, 9*s)                
      S  }
#bimodel distribution:0.5dnorm(1,1)+0.5dnorm(9,1)
for(i in 1:m){
x <- rt(n1, df=1)
y <- rnd(n2)
z <- c(x, y) 
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.etest(z,sizes=N,R=R)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=999,seed=i*12345)$p.value
}
pow <- colMeans(p.values<alpha)
names(pow)<-c('NN','energy','Ball')
```
```{r results='asis'}
stargazer(pow,type='html')
```
Choose bimodel distribution:$0.5dnorm(1,1)+0.5dnorm(9,1)$.Energy is the most powerful, while the other two are also powerful. The reason why each value of the result is big is that t distribution with 1 df (heavy-tailed distribution) differs from bimodel distribution greatly.  
  
4.Unbalanced samples (say, 1 case versus 10 controls)
```{r warning=FALSE}
edist.2 <- function(x, ix, sizes) {
# computes the e-statistic between 2 samples
# x: Euclidean distances of pooled sample
# sizes: vector of sample sizes
# ix: a permutation of row indices of x
dst <- x
n1 <- sizes[1]
n2 <- sizes[2]
ii <- ix[1:n1]
jj <- ix[(n1+1):(n1+n2)]
w <- n1 * n2 / (n1 + n2)
# permutation applied to rows & cols of dist. matrix
m11 <- sum(dst[ii, ii]) / (n1 * n1)
m22 <- sum(dst[jj, jj]) / (n2 * n2)
m12 <- sum(dst[ii, jj]) / (n1 * n2)
e <- w * ((m12 + m12) - (m11 + m22))
return (e)
}
eqdist.energy <- function(z,sizes,k){
boot.obj <- boot(data = as.matrix(dist(z)), statistic = edist.2,
sim = "permutation", R = 999, sizes = sizes)
ts <- c(boot.obj$t0,boot.obj$t)
p.value <- mean(ts>=ts[1])
list(statistic=ts[1],p.value=p.value)
}
```
```{r warning=FALSE}
n1<-100;n2<-10;n <- n1+n2; N = c(n1,n2)
#n1 is ten times larger than n2
m<-100;set.seed(12345)
for(i in 1:m){
x <- rnorm(n1,0,3)#N(0,3)
y <- rnorm(n2)#N(0,1)
z <- c(x, y) 
p.values[i,1] <- eqdist.nn(z,N,k)$p.value
p.values[i,2] <- eqdist.energy(z,N,k)$p.value
p.values[i,3] <- bd.test(x=x,y=y,R=99,seed=i*12345)$p.value
}
pow <- colMeans(p.values<alpha)
names(pow)<-c('NN','energy','Ball')
```
```{r results='asis'}
stargazer(pow,type='html')
```
Under this circumstance: $X\sim N(0,3),Y\sim N(0,1)$(1 case versus 10 controls), Ball is the most powerful, due to Ball could be more powerful for non-location family distribution(eg:the variances are different).  
  
## 2020-11-17  
  

## 9.4

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.The standard Laplace distribution has density 
$$f(x)=\dfrac{1}{2}e^{−|x|},x∈R$$
## Answer 1
```{r}
rw.Metropolis <- function(sigma, x0, N) {
x <- numeric(N)
x[1] <- x0
u <- runif(N)
k <- 0
for (i in 2:N) {
y <- rnorm(1, x[i-1], sigma)
if (u[i] <= (exp(abs(x[i-1])-abs(y))))
x[i] <- y else {
x[i] <- x[i-1]
k <- k + 1
}
}
return(list(x=x, k=k))
}
N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 50
rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)
#acceptance rates of each chain
print(1-c(rw1$k/N, rw2$k/N, rw3$k/N, rw4$k/N))
```
```{r}
x<-1:2000
#par(mfrow=c(2,2))
plot(x, rw1$x, type="l", sub="σ=0.05", xlab='',ylab="x")
plot(x, rw2$x, type="l", sub="σ=0.5", xlab='',ylab="x")
plot(x, rw3$x, type="l", sub="σ=2", xlab='',ylab="x")
plot(x, rw4$x, type="l", sub="σ=16", xlab='',ylab="x")
```
  
According to the picture, we find that the chain converges more quickly when $\sigma$ is larger. But if $\sigma$ is too large, the acceptance rates will be low, which means the simulation is inefficient.
```{r include=FALSE,warning=FALSE}
library(jmuOutlier)
library(flextable)
library(xtable)
```
```{r results='asis'}
a <- c(.05, seq(.1, .9, .1), .95)
Q <- qlaplace(a)
rw <- cbind(rw1$x, rw2$x, rw3$x, rw4$x)
mc <- rw[501:N, ]
Qrw <- apply(mc, 2, function(x) quantile(x, a))
colnames(Qrw)<-c('σ=0.05','σ=0.5','σ=2','σ=16')
xtable_to_flextable(xtable(cbind(Q, Qrw)))
```
If the Qrw is close to Q, then the simulation of the distribution is good.

## Question 2
For Exercise 9.4, use the Gelman-Rubin method to monitor
convergence of the chain, and run the chain until it converges approximately to the target distribution according to $\hat R < 1.2$.

## Answer 2
```{r}
Gelman.Rubin <- function(psi) {
# psi[i,j] is the statistic psi(X[i,1:j])
# for chain in i-th row of X
psi <- as.matrix(psi)
n <- ncol(psi)
k <- nrow(psi)
psi.means <- rowMeans(psi) #row means
B <- n * var(psi.means) #between variance est.
psi.w <- apply(psi, 1, "var") #within variances
W <- mean(psi.w) #within est.
v.hat <- W*(n-1)/n + (B/n) #upper variance est.
r.hat <- v.hat / W #G-R statistic
return(r.hat)
}
```
```{r}
set.seed(123)
sigma1 <-0.5 #parameter of proposal distribution
sigma2 <-2 #parameter of proposal distribution
k <- 4 #number of chains to generate
n <- 15000 #length of chains
b <- 1000 #burn-in length
#choose overdispersed initial values
x0 <- c(-10, -5, 5, 10)
#generate the chains
X <- matrix(0, nrow=k, ncol=n)
Y <- matrix(0, nrow=k, ncol=n)
for (i in 1:k){
  X[i, ] <- rw.Metropolis(sigma1, x0[i], n)$x
  Y[i, ] <- rw.Metropolis(sigma2, x0[i], n)$x
}
#compute diagnostic statistics
psi1 <- t(apply(X, 1, cumsum))
psi2 <- t(apply(Y, 1, cumsum))
for (i in 1:nrow(psi1))
psi1[i,] <- psi1[i,] / (1:ncol(psi1))
for (i in 1:nrow(psi2))
psi2[i,] <- psi2[i,] / (1:ncol(psi2))
#plot psi for the four chains
#par(mfrow=c(2,2))
for (i in 1:k)
plot(psi1[i, (b+1):n], type="l",
xlab=i, ylab=bquote(psi1))
par(mfrow=c(1,2)) #restore default
#plot the sequence of R-hat statistics
rhat <- rep(0, n)
for (j in (b+1):n)
rhat[j] <- Gelman.Rubin(psi1[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R",sub='σ=0.5')
abline(h=1.2, lty=2)
rhat <- rep(0, n)
for (j in (b+1):n)
rhat[j] <- Gelman.Rubin(psi2[,1:j])
plot(rhat[(b+1):n], type="l", xlab="", ylab="R",sub='σ=2')
abline(h=1.2, lty=2)
```

We choose $\sigma=0.5,2$ to compare. From the pictures, We find that each $\hat R$ converges, and $\hat R < 1.2$ quickly when $\sigma$ is larger.

## 11.4
Find the intersection points $A(k)$ in $(0,\sqrt k)$ of the curves
$$S_{k−1}(a)=P(t(k − 1)>\sqrt{\dfrac{a^2(k−1)}{k−a^ 2}})$$
and
$$S_k(a)=P(t(k)>\sqrt{\dfrac{a^2k}{k+1−a^2}})$$
for$k=4:25,100,500,1000$, where$t(k)$is a Student $t$ random variable with $k$ degrees of freedom. (These intersection points determine the critical values for a $t$-test for scale-mixture errors proposed by Szekely [260].)
## Answer 3
```{r warning=FALSE}
f <- function(x,k) pt((x^2*k/(k+1-x^2))^0.5,df=k)-pt((x^2*(k-1)/(k-x^2))^0.5,df=k-1)
k<-c(4:25,100,500,1000)
t<-round(k^0.5,3)
res<-numeric(length(k))
for(i in 1:25)
  res[i] <- uniroot(f,c(1,2),k=k[i])
unlist(res)
```
As the picture of f showed below, we find that it is gradually approach to 0 when k is large. If we choose the upper as $\sqrt k$, the result will be bad, because uniroot can not identify small difference. By observing the curves, we find roots are between $(1,2)$, thus, we choose this as the initial interval.
```{r}
#par(mfrow=c(2,2))
curve(f(x,4),xlim=c(0,5),sub='k=4:25')
for(i in 5:25)
curve(f(x,i),xlim=c(0,5),add=TRUE,col=i)
abline(h=0,col='red')
curve(f(x,100),xlim=c(0,10),sub='k=100')
abline(h=0,col='red')
curve(f(x,500),xlim=c(0,23),sub='k=500')
abline(h=0,col='red')
curve(f(x,1000),xlim=c(0,32),sub='k=1000')
abline(h=0,col='red')
```
  
## 2020-11-24
 
## Question 1
| Genotype  | AA | BB | OO | AO | BO | AB | Sum |
|  ----  | ----  |  ----  | ----  |  ----  | ----  |  ----  | ----  |
|Frequency  | p^2 | q^2 | r^2 | 2pr | 2qr | 2pq |1 |
| Count   | $n_{AA}$ | $n_{BB}$ | $n_{OO}$ | $n_{AO}$ | $n_{BO}$ | $n_{AB}$ | n |
Observed data: $n_{A·} = n_{AA} + n_{AO} = 444$ (A-type),
$n_{B·} = n_{BB} + n_{BO} = 132$ (B-type), $n_{OO} = 361$ (O-type),$n_{AB}$ = 63 (AB-type).      
1.Use EM algorithm to solve MLE of p and q (consider missing data $n_{AA}$ and $n_{BB}$ ).  
2.Record the values of p and q that maximize the conditional likelihood in each EM steps, calculate the corresponding log-maximum likelihood values (for observed data), are they increasing?

## Answer 1
```{r warning=FALSE}
#Use EM algorithm to solve MLE of p and q
N <- 1000 #max. number of iterations
L <- c(.2, .2) #initial est. for lambdas
tol <- .Machine$double.eps^0.5
L.old <- L + 1
value<-L
for (j in 1:N) {
p<-507*2/(2-L[1])*(1-L[2])/(1298+507*L[1]/(2-L[1])-195*L[2]/(2-L[2]))
q<-195*2/(2-L[2])*(1-L[1])/(1298-507*L[1]/(2-L[1])+195*L[2]/(2-L[2]))
L <- c(p,q) #update
value<-rbind(value,L)#record every time value 
if (sum(abs(L - L.old))<tol) break
L.old <- L
}
print(list(L, iter = j, tol = tol))
```
```{r}
#calculate the corresponding log-maximum likelihood values (for observed data)
p<-value[,1]
q<-value[,2]
r<-1-p-q
cllv<-444*log(p^2+2*p*r)+132*log(q^2+2*q*r)^132+361*2*log(r)+63*log(2*p*q)
cllv
```
They are increasing.

## Exercise 3(Advanced R P204)
Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

## Answer 2
```{r}
formulas <- list(
mpg ~ disp,
mpg ~ I(1 / disp),
mpg ~ disp + wt,
mpg ~ I(1 / disp) + wt
)
```
```{r}
##Use loops to fit linear models to the the mtcars
attach(mtcars)
x<-cbind(disp,I(1 / disp),disp + wt,I(1 / disp) + wt)
out <- vector("list", 4)
for (i in 1:4) {
out[[i]] <- lm(mpg~x[,i])
}
out
```
```{r}
##Use lapply to fit linear models to the the mtcars
x<-as.data.frame(x)
lapply(x,function(x) lm(mpg~x))
detach(mtcars)
```


## Exercise 3(Advanced R P213-214)
The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial.  
Extra challenge: get rid of the anonymous function by using [[ directly. 

## Answer 3
```{r}
trials <- replicate(
100,
t.test(rpois(10, 10), rpois(7, 10)),
simplify = FALSE
)
# Use sapply() and an anonymous function to extract the p-value from every trial
sapply(trials,function(x) x$p.value)
```
```{r}
#get rid of the anonymous function by using [[ directly
sapply(trials,'[[','p.value')
```

 
## Exercise 6(Advanced R P213-214)
Implement a combination of Map() and vapply() to create an
lapply() variant that iterates in parallel over all of its inputs and stores its outputs in a vector (or a matrix). What arguments should the function take?

## Answer 4
```{r include=FALSE}
library(MAP)
```

```{r}
newlist <- list(cars,mtcars)
lapply(newlist, function(x) vapply(x, var, numeric(1)))
```
```{r}
lapply_new <- function(data, f, index, simplify = FALSE){
  out <- Map(function(x) vapply(x, f, index), data)
  unlist(out,recursive = FALSE)
}
lapply_new(newlist, var, numeric(1))
```
We output it as a vector, the result is the same as the former.
  
## 2020-12-02  
  
## 9.4

Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.The standard Laplace distribution has density 
$$f(x)=\dfrac{1}{2}e^{−|x|},x∈R$$

## Answer 1
```{r warning=FALSE}
library(Rcpp)
library(StatComp20093)
```

```{r}
N <- 2001
sigma <- c(.05, .5, 2, 16)
x0 <- 50
rw1 <- rwMetropolis(sigma[1], x0, N)
rw2 <- rwMetropolis(sigma[2], x0, N)
rw3 <- rwMetropolis(sigma[3], x0, N)
rw4 <- rwMetropolis(sigma[4], x0, N)
#acceptance rates of each chain, the first of the chain stores the numbers of acceptance.
print(1-c(rw1[[1]], rw2[[1]], rw3[[1]], rw4[[1]])/N)
```
```{r}
x<-1:2000
#par(mfrow=c(2,2))
plot(x, unlist(rw1[-1]), type="l", sub="σ=0.05", xlab='',ylab="x")
plot(x, unlist(rw2[-1]), type="l", sub="σ=0.5", xlab='',ylab="x")
plot(x, unlist(rw3[-1]), type="l", sub="σ=2", xlab='',ylab="x")
plot(x, unlist(rw4[-1]), type="l", sub="σ=16", xlab='',ylab="x")
```


## Question 2
Compare the corresponding generated random numbers with
those by the R function you wrote before using the function “qqplot”.

## Answer 2
```{r}
#generated by the R function
N <- 2000
rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (exp(abs(x[i-1])-abs(y))))
      x[i] <- y else {
        x[i] <- x[i-1]
        k <- k + 1
      }
  }
  return(list(x=x, k=k))
}
rw11 <- rw.Metropolis(sigma[1], x0, N)
rw12 <- rw.Metropolis(sigma[2], x0, N)
rw13 <- rw.Metropolis(sigma[3], x0, N)
rw14 <- rw.Metropolis(sigma[4], x0, N)
#acceptance rates of each chain
print(1-c(rw11$k/N, rw12$k/N, rw13$k/N, rw14$k/N))
```
```{r}
#par(mfrow=c(2,2))
qqplot(unlist(rw1[-1]),rw11$x,main='σ=0.05',xlab='C',ylab='R')
qqplot(unlist(rw2[-1]),rw12$x,main='σ=0.5',xlab='C',ylab='R')
qqplot(unlist(rw3[-1]),rw13$x,main='σ=2',xlab='C',ylab='R')
qqplot(unlist(rw4[-1]),rw14$x,main='σ=16',xlab='C',ylab='R')
```

## Question 3
Campare the computation time of the two functions with the
function “microbenchmark”.

## Answer 3
```{r}
library(microbenchmark)
ts1 <- microbenchmark(rwC=rwMetropolis(sigma[1], x0, N),
rwR=rw.Metropolis(sigma[1], x0, N))
ts2 <- microbenchmark(rwC=rwMetropolis(sigma[1], x0, N),
rwR=rw.Metropolis(sigma[2], x0, N))
ts3 <- microbenchmark(rwC=rwMetropolis(sigma[1], x0, N),
rwR=rw.Metropolis(sigma[3], x0, N))
ts4 <- microbenchmark(rwC=rwMetropolis(sigma[1], x0, N),
rwR=rw.Metropolis(sigma[4], x0, N))
ts<-list(ts1,ts2,ts3,ts4)
lapply(ts,function(x) summary(x)[,c(1,3,5,6)])
```
  
Comments:     
Compare the qqplot, it is similar to y=x, this is rational, because the samplers generated from the same distribution.  
Compare the computation time of the two functions, the procession using C consumes less time.

### Thanks for your review!





